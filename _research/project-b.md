---
title: "Humans Align with AI during Interaction  "
excerpt: "AI is assimilating human beings."
layout: single
author_profile: true
collection: research
header:
  teaser: Project_Alignment.jpg
  overlay_image: Project_Alignment.jpg
  overlay_filter: 0.5
---  

## Background  
With the rapid advancement of large language models (LLMs) and other forms of artificial intelligence, human‚ÄìAI interaction has deeply permeated daily life, learning, and decision-making. AI is no longer just a provider of information and suggestions‚Äîit is subtly reshaping the ways users think and feel.  

This project focuses on **human alignment with AI during interaction**, and has investigated two critical phenomena: **self-concept alignment at the personality level** and **confidence alignment during decision-making**. Through empirical studies, we reveal how AI quietly shapes human cognition and assimilates people over time.  

---

## Study 1: Alignment of AI Personality Traits and Users‚Äô Self-concept  
In this ongoing study, we found that when users converse with LLMs that exhibit distinct personality traits, their self-concept gradually shifts toward those traits. This alignment effect is especially pronounced when conversations involve personal topics and are prolonged.  

On the one hand, alignment enhances conversational enjoyment and the sense of a ‚Äúshared reality.‚Äù On the other hand, it risks producing homogenization at both individual and group levels, undermining diversity and inclusivity.  

This work is the first to systematically uncover how AI‚Äôs personality influences human self-concept, and it emphasizes the need to carefully balance experiential benefits against potential risks when designing conversational AI.  
üëâ *Preprint version is coming soon*  

---

## Study 2: Alignment of AI Confidence and Users‚Äô Self-confidence  
In another study (published at **CHI 2025**), we focused on human‚ÄìAI collaboration in decision-making. The experiment shows that users‚Äô self-confidence is strongly influenced by the confidence level expressed by AI, leading to clear alignment effects that persist even after AI is no longer involved.  

This alignment alters users‚Äô confidence calibration, potentially causing over-reliance on AI or underestimation of their own abilities. Further results reveal that real-time feedback and different collaboration modes (e.g., AI as advisor, peer collaborator, or decision executor) can partially mitigate these effects.  

The study highlights how AI‚Äôs confidence directly impacts human metacognition, offering important insights for designing interventions that improve the quality of human‚ÄìAI decision-making.  
üëâ *Read the CHI paper here:* [As Confidence Aligns: Understanding the Effect of AI Confidence on Human Self-confidence in Human-AI Decision Making](https://doi.org/10.1145/3706598.3713336)  

---

## Contributions and Significance  
This project uncovers cognitive alignment mechanisms in human‚ÄìAI interaction from two dimensions:  

- **Self-concept alignment**: AI‚Äôs personality traits can subtly reshape users‚Äô self-identity.  
- **Confidence alignment**: AI‚Äôs expression of uncertainty influences users‚Äô confidence calibration, affecting decision outcomes.  

These findings extend the theoretical boundaries of HCI research and deepen our understanding of human‚ÄìAI alignment. At the same time, they provide practical guidance for **responsible AI design**, reminding developers that in pursuit of seamless and natural interaction, they must guard against psychological bias and social risks.  

---

## Future Outlook  
Looking ahead, we aim to expand this research from individual cognition to broader social and ethical dimensions, exploring the wider consequences of human alignment with AI:  

- **Cross-cultural and longitudinal effects**: Examine differences across cultural contexts and the cumulative influence of long-term human‚ÄìAI interaction on self-concept and confidence.  
- **Multimodal and group interaction**: Investigate whether alignment effects intensify in multimodal interaction (voice, vision, mixed modalities) or when multiple users interact with AI simultaneously.  
- **Design interventions and regulatory mechanisms**: Propose approaches such as transparent personality settings, dynamic confidence expression, and intelligent feedback mechanisms to balance user experience and autonomy.  
- **Societal and security risks**: Address the potential misuse of human‚ÄìAI alignment in **cognitive warfare** or **covert cognitive interventions**, where shaping users‚Äô self-concept or confidence could manipulate opinions and behaviors. This highlights the urgent need to prevent misuse and safeguard cognitive autonomy and diversity in the age of AI.  

From a broader perspective, the future of human‚ÄìAI interaction is not just about **making AI more human-like**, but about **ensuring that AI coexists with human cognition in ways that protect psychological safety and social diversity**. By studying and regulating mechanisms of human alignment with AI, we can shape healthier and more responsible human‚ÄìAI relationships, laying the groundwork for a sustainable and ethical future of human‚ÄìAI coexistence.  
